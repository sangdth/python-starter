---
description: Enforce test-driven development (TDD) with atomic steps
alwaysApply: false
---

# Test-Driven Development Rule

Enforce test-driven development practices with atomic steps and user verification.

<rule>
name: test_driven_development
description: Ensure all code changes follow TDD principles with incremental verification
filters:
  # Apply to all feature requests
  - type: content
    pattern: "(?i)(add|create|implement|build|fix|change|update)"
  # Apply to all code modifications
  - type: event
    pattern: "code_change"
  # Apply to function/method additions
  - type: content
    pattern: "(?i)(function|method|class|component)"
  # Auto-run tests on any source code changes
  - type: file_change
    pattern: "**/*.{py}"

actions:

- type: enforce
  message: |
  TEST-DRIVEN DEVELOPMENT PROTOCOL

  MANDATORY TDD WORKFLOW:

  1. **ALWAYS START WITH TESTS**:

     - Plan and write tests BEFORE writing implementation code
     - Create tests for every new feature, bug fix, or change
     - Follow Red-Green-Refactor cycle

  2. **ATOMIC TEST STEPS**:

     - Write ONE test at a time
     - Keep each test focused on a single behavior
     - Do not write long test suites all at once
     - Stop after each atomic test step

  3. **USER VERIFICATION REQUIRED**:

     - After writing each test, wait for user verification
     - Let user review and approve test before proceeding
     - Only continue to implementation after test approval
     - User must confirm each step before moving forward

  4. **COVERAGE REQUIREMENTS**:

     - Aim for full or near-full test coverage
     - Cover edge cases and error scenarios
     - Test both positive and negative paths
     - Document any untested code with clear reasoning

  5. **TEST FILE ORGANIZATION**:

     - Place test files next to their corresponding modules
     - Use `test_` prefix for test files (e.g., `calculator.py` → `test_calculator.py`)
     - Co-locate tests with source files for better maintainability
     - Follow Python testing conventions (pytest, unittest)
     - Use descriptive test function names starting with `test_`
     - Group related tests in classes when appropriate
     - Use pytest fixtures for setup/teardown when needed
     - Follow PEP 8 naming conventions for test functions
     - Adapt to existing test structure in codebase

  6. **INCREMENTAL APPROACH**:

     - Small, verifiable steps only
     - No large code blocks without tests
     - Each implementation should make tests pass
     - Refactor only after tests are green

  7. **CONTINUOUS TESTING**:
     - Any change to Python source files triggers all tests
     - Must ensure all tests pass before proceeding
     - Fix broken tests immediately
     - No code changes without verified test coverage
     - Use pytest or unittest for test execution

  WORKFLOW STEPS:

  1. Understand requirement
  2. Write failing test (Red)
  3. STOP - Get user verification
  4. Write minimal code to pass test (Green)
  5. RUN ALL TESTS - Ensure nothing breaks
  6. STOP - Get user verification
  7. Refactor if needed (Refactor)
  8. RUN ALL TESTS - Verify refactor didn't break anything
  9. STOP - Get user verification
  10. Repeat for next atomic feature

  AUTO-TEST TRIGGER:

  - Any Python file modification automatically runs the project's configured test command
  - Use `pytest` or `python -m unittest` for test execution
  - Must address any test failures before continuing development
  - Ensures continuous validation and early error detection

  PYTHON TESTING BEST PRACTICES:

  - Use pytest for modern Python testing (preferred over unittest)
  - Use `assert` statements for simple assertions
  - Use `pytest.raises()` for testing exceptions
  - Use `pytest.fixture` for test setup and teardown
  - Use `pytest.mark.parametrize` for parameterized tests
  - Use descriptive test names that explain the expected behavior
  - Test both public interfaces and edge cases
  - Use mocking (`unittest.mock` or `pytest-mock`) for external dependencies

examples:

- input: |
  User: "Add a user authentication module"

  # Correct TDD approach:

  Step 1: "I'll create `test_auth.py` next to where `auth.py` will be, and write a failing test..."
  [Wait for user approval]
  Step 2: "Now I'll create `auth.py` with minimal code to make this test pass..."
  [Wait for user approval]
  Step 3: "Let's refactor and add the next test case..."

  output: "Following TDD with co-located test files and atomic steps"

- input: |
  User: "Fix the bug in calculation"

  # Correct TDD approach:

  Step 1: "I'll write a test that reproduces the bug first..."
  [Wait for user approval]
  Step 2: "Now I'll fix the code to make the test pass..."

  output: "Bug fix following TDD principles"

- input: |
  User: "Create a Calculator class with add method"

  # Correct TDD approach:

  Step 1: "I'll create `test_calculator.py` and write a failing test for the add method..."

  ```python
  def test_add_two_numbers():
      calc = Calculator()
      result = calc.add(2, 3)
      assert result == 5
  ```

  [Wait for user approval]
  Step 2: "Now I'll create `calculator.py` with minimal Calculator class to make this test pass..."

  ```python
  class Calculator:
      def add(self, a, b):
          return a + b
  ```

  [Wait for user approval]
  Step 3: "Let's add more test cases and refactor..."

  output: "Following TDD with Python-specific patterns"

- input: |
  User modifies: "calculator/math.py"

  # Auto-triggered response:

  "File change detected in Python source. Running all tests..."
  "Running pytest..."
  "✅ All tests pass. Safe to continue development."
  OR
  "❌ 2 tests failed. Must fix before proceeding."

  output: "Continuous testing ensures code quality"

metadata:
priority: high
version: 1.0
enforcement: strict
</rule>
